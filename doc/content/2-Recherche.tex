\chapter{Recherche}

In diesem Kapitel werden die Rahmenbedingungen, die verschiedenen Testing Frameworks (Service-Anbieter) sowie die Zielgruppen recherchiert.

\section{Rahmenbedingungen}
\label{sec:Recherche:Rahmenbedingungen}
Getestet werden soll die Webseite der Firma travelwindow AG, welche unter http://www.travel.ch erreichbar ist. Die Seite ist mit C\# implementiert, weshalb auch die Tests in dieser Programmiersprache umgesetzt werden sollen.

\subsection{Infrastruktur}
\label{sec:Recherche:Rahmenbedingungen:Infrastruktur}
Es gibt folgende drei Umgebungen:
\begin{itemize}
\item Test
\item Quality
\item Production
\end{itemize}
Bei der Entwicklung wird jeder Checkin in das Versionierungssystem (siehe \cref{sec:Recherche:Rahmenbedingungen:Tooling} \nameref{sec:Recherche:Rahmenbedingungen:Tooling}) automatisch auf die Test-Umgebung aufgespielt. Sobald ein Stand erreicht ist, welcher vom Business der travelwindow AG getestet werden soll, wird diese Version auf die Quality-Umgebung geladen. Sobald diese fertig mit dem Testen sind, wird diese Version auf die Production-Umgebung gespielt und ist damit Live.

\subsection{Tooling}
\label{sec:Recherche:Rahmenbedingungen:Tooling}
Der Sourcecode der Softare ist auf einem eigenen \gls{svn}\footcite{Apache_Subversion_2015-07-26} versioniert. Es ist jedoch eine Umstellung auf GIT\footcite{Git_2015-07-26} geplant, weshalb neuer Sourcecode fortan nur noch auf dem GIT Hosting Dienst BitBucket\footcite{Git_and_Mercurial_code_management_for_teams_2015-07-26} gespeichert werden soll.

Als Continuous Integration Server wird Bamboo\footcite{Bamboo_2015-07-26} eingesetzt. Dieser kompiliert den Sourcecode und ist dafür verantwortlich, eine Version auf die Umgebungen Test, Quality und Production zu laden.

Das Tooling ist genauer beschrieben im \cref{sec:umsetzung:infrastruktur} \nameref{sec:umsetzung:infrastruktur}.

\section{Testing Framework}
\label{sec:Recherche:TestingFrameworks}
Der defacto Standart für Integrationstests von Benutzeroberfläche ist Selenium\footcite{Selenium_-_Web_Browser_Automation_2015-07-26}\footcite{Happy_10th_Birthday_Selenium_ThoughtWorks_2015-07-26}. Der Code für dieses Framework kann in C\# programmiert werden, welcher einen Webbrowser startet und Benutzereingaben simuliert. Dadurch können repetitive Tests automatisiert und auf verschiedenen Browsern durchgeführt werden.

\section{Lösungswege: Lokal oder Service-Anbieter}
\label{sec:Recherche:loesungswege}
Die Tests können entweder auf einer eigens aufgesetzten Umgebung durchgeführt, oder an einen Service ausgelagert werden, welcher die Tests ausführt. 
Der Vorteil solcher Services liegt darin, dass sie virtuelle Umgebungen für die Durchführung der Tests zur Verfügung stellen. Diese sind mit verschiedenen Betriebssystemen und Browser Konfigurationen ausgestattet. Einige Beispiele solcher Zusammenstellungen:
\begin{itemize}
\item Windows 7, Firefox 39
\item Windows 8.1, Firefox 39
\item OS X Yosemite, Safari 8
\item IPhone, iOS 8.4
\item Android 4.4, Samsung Galaxy S4
\end{itemize}

Als Service-Anbieter Tests wurden zwei Kandidaten näher in Betracht gezogen. SauceLabs\footcite{Sauce_Labs_2015-07-26} und CrossBrowserTesting\footcite{Cross_Browser_Testing_2015-07-26}, da diese zwei als Hauptsponsor des Selenium Projekts auf deren Homepage aufgeführt wurden\footcite{Seleniumhq}

Beide Anbieter verfügen über 500 verschiedene Betriebsystem/Browser Konfigurationen\footcite{Platforms_2015-07-26}\footcite{OS_Browser_Configurations_for_Cross_Browser_Compatibility_Testing_2015-07-26}. Beide können automatisierte Tests ausführen und liefern als Resultat ein Video der Durchführung sowie Screenshots der einzelnen Tests.

Bezahlt werden die Service-Anbieter pro Minute, die ein Test auf ihren System läuft. Vom Preis her unterscheiden sie sich nicht markant\footcite{Sauce_Labs_Pricing_2015-07-26}\footcite{Test_your_site_on_all_browsers_2015-07-26}. SauceLabs hat das kleinste Packet mit 120 Minuten automatisierten Tests für \$12/Monat. Beide Anbieter verrechnen für 10 Stunden Testing \$49/Monat. Beim teureren Packet für \$149/Monat bietet CrossBrowserTesting mit 2000- mehr als Sauclabs mit 1800 Minuten.

Um sich zwischen den beiden Varianten, der lokalen Ausführung der Test und deren Auslagerung an einen Serivce-Anbieter, zu entscheiden, wurde ein Prototyp entwickelt welcher im nächsten Kapitel vorgestellt wird.

\section{Lösungswege: Prototyp}
Der Prototyp soll eine Grundlage bieten, um sich zwischen den vorgestellten Lösungswegen (siehe \cref{sec:Recherche:loesungswege} \nameref{sec:Recherche:loesungswege}) zu entscheiden. Dieser kann die Tests lokal durchführen, oder sie an einen Service-Anbieter auslagern.

Erwartet wird, dass das vorbereiten für die lokale Ausführung mehr Zeit in Anspruch nimmt, da zuerst die Treiber für die verschiedenen zu Browser eingerichtet werden müssen. Es werden sowohl Desktop wie auch Mobile Internetclients unterstützt werden. Das Aufsetzten und Pflegen verschiedener Servern mit verschiedenen Betriebssystemen, Browsern und Emulatoren für mobile Endgeräte ist aufwändig und benötigt viel Knowhow. Auch deren Pflege sollte mehr Zeit beanspruchen. Dafür ist die Ausführung der Tests kostenlos.

Bei den Service-Anbietern sollte die Ausführung einfacher einzurichten sein, da die Browser schon vorkonfiguriert auf deren Systemen installiert sind. Jedoch wird die Testausführung in Rechnung gestellt. 

Die obigen Annahmen sollten durch den Prototyp bestätigt oder wiederlegt werden. Der Quellcode des Programms ist im BitBucket unter \url{https://bitbucket.org/soultemptation/tw-systemtests-prototype} einsehbar.

\subsection{Auswertung}
Getestet wurde die lokale Ausführung auf dem Server mit dem Firefox Browser, und die Anbindung der Service-Anbieter SauceLabs und CrossBrowserTesting. 

\subsubsection{Aufsetzen}
Das Aufsetzen des Firefox Browsers war einfach jedoch Zeitintensiv. Java muss auf dem Server vorhanden sein und der Selenium RC Server muss installiert werden. Möchte man die Test zusätzlich auf anderen Betriebsystemen ausführen, so steigt der Aufwand rasant an, da jede Betriebsystem/Browser Konfiguration muss separat aufgesetzt werden\footcite{Selenium_Linux}. An Aufwand hat es drei Stunden Arbeit und zwei Stunden Recherche gekostet.

Bei den Service Anbietern kann man beim Aufruf mitgeben, welche Betriebsystem/Browser kombination man testen möchte. Der folgende Code ist ein Ausschnitt aus dem Prototypen:

\begin{lstlisting}
    var commandExecutorUri = new Uri("`http://ondemand.saucelabs.com/wd/hub"');
    var desiredCapabilites = GetDesirecCapabilities();

    // set operatingsystem/browser configuration
    desiredCapabilites.SetCapability("`platform"', "`Windwos8"'); // operating system to use
    desiredCapabilites.SetCapability("`browser"', "`Firefox25"'); // supply sauce labs username
\end{lstlisting}
Auf Zeile 5 und 6 wird das Betriebsystem beziehungsweise der Browser gesetzt. Um weitere Konfigurationen zu testen müssen lediglich diese Werte angepasst werden.

\subsubsection{Zeit}
Der Test startet die travel.ch Seite und gibt einen Wert in das Suchformular ein. Dann wartet es bis Suchvorschläge angezeigt werden. Der Test ist erfolgreich wenn mindestens ein Vorschlag vorhanden ist.

Der Test wurde zehnmal lokal sowie beim Service-Anbieter ausgeführt:
\begin{table}[H] 
	\caption{Testlaufzeiten}
	\centering
	\rowcolors{1}{tablebodycolor}{tablerowcolor}
		
	\begin{tabular}{ | c | c | c | c |} 
		\hline 
			\rowcolor{tableheadcolor}
				 \bfseries \# & 
				 \bfseries Lokal & 
				 \bfseries SauceLabs &
				 \bfseries CrossBrowserTesting \\ \hline 
			1 & 16.42 Sekunden & 59.52 Sekunden & 56.21 Sekunden \\ \hline 
			2 & 18.21 Sekunden & 55.21 Sekunden & 55.78 Sekunden \\ \hline 
			3 & 17.47 Sekunden & 56.87 Sekunden & 59.21 Sekunden \\ \hline 
			4 & 15.98 Sekunden & 63.74 Sekunden & 60.55 Sekunden \\ \hline 
			5 & 15.99 Sekunden & 51.11 Sekunden & 63.47 Sekunden \\ \hline 
			6 & 16.21 Sekunden & 55.47 Sekunden & 62.69 Sekunden \\ \hline 
			7 & 17.01 Sekunden & 56.87 Sekunden & 59.74 Sekunden \\ \hline 
			8 & 21.21 Sekunden & 57.31 Sekunden & 55.28 Sekunden \\ \hline 
			9 & 16.78 Sekunden & 53.88 Sekunden & 60.77 Sekunden \\ \hline 
			10 & 16.55 Sekunden & 57.29 Sekunden & 58.52 Sekunden \\ \hline 
	\end{tabular} 
\end{table}
Durchschnittlich macht dass eine Laufzeit von 17.81 Sekunden für die lokale Ausführung, 56.73 bei SauceLabs und 59.22 bei CrossBrowserTesting.

Die Service-Anbieter ermöglichen es, zwei Tests parallel auszuführen. Dadurch kann die Laufzeit eines einzelnen Tests nicht verändert werden, die ganze Test-Suite wird jedoch in der Hälfte der Zeit abgearbeitet.

\subsubsection{Kosten}
Die Kosten sind schwer vergleichbar. Bei den Service-Anbieter wird nach der Laufzeit der Tests abgerechnet (siehe \cref{sec:Recherche:loesungswege} \nameref{sec:Recherche:loesungswege}). Für die lokale Ausführung wird die Arbeitszeit bezahlt, welche für die Installation und Wartung der Server und Browser benötigt wird. Zusätzlich fallen Lizenzkosten an für die zu installierende Software auf den Servern und es wird teures Expertenwissen in diesem Bereich benötigt. Im speziellen wenn auf mobilen Endgeräten getestet werden muss.

\subsubsection{Entscheid}
Das Aufsetzten und die Pflege der Umgebung ist eine klare stärke der Service-Anbieter. Dafür haben sie die Schwäche bei den Testlaufzeiten. 

Die lokale Ausführung hingegeben benötigt für ersteres mehr Aufwand, dafür laufen die Tests mit einem Faktor 3 schneller.

Nach Absprache mit dem Business der travelwindow AG wurde entschieden, dass der Lösungsweg mit den  Service-Anbieter eingeschlagen werden soll. Der Aufwand zum Aufsetzen und Pflege der Umgebung ist geringer und die Kosten können besser kalkuliert werden. Sollte die Laufzeiten ein Problem darstellen, so kann eine parallele Ausführung der Tests in Betracht gezogen werden. 

\section{Zielgruppe}
\label{sec:Recherche:Zielgruppe}
Die Zielgruppe sind die Benutzer der Webseite, beziehungsweise die verschiedenen Browser, welche sie verwenden, sowie die Destinationen für die sie eine Reise buchen möchten. Denn für diese müssen Tests geschrieben werden.

\subsection{Browserverteilung}
Auf travel.ch wird Google Analytics eingesetzt. Die Auswertung des letzten halben Jahres (25.02.2015 - 25.07.2015) ergab, dass folgende Browserversionen am meisten verwendet wurden:
\begin{itemize}
\item Safari: 32.61\%
	\begin{itemize}
			\item 8.0: 53.40\%
			\item 7.0: 12.62\%
	\end{itemize}
\item Internet Explorer: 26.62\%
	\begin{itemize}
			\item 11.0: 74.23\%
			\item 9.0: 13.95\%
	\end{itemize}
\item Chrome: 21.14\%
	\begin{itemize}
			\item 40.0: 19.24\%
			\item 41.0: 15.82\%
	\end{itemize}
\item Firefox: 14.22\%
	\begin{itemize}
			\item 36.0: 25.95\%
			\item 37.0: 21.53\%
	\end{itemize}
\end{itemize}

\subsection{Top 10 Destinationen}
Die Webseite ist in drei Bereiche aufgeteilt: Citytrip (Flug \& Hotel), Flug, Hotel. Folgend nun die zehn meist gesuchten Destinationen:
\begin{table}[H] 
	\caption{Top 10 Destinationen}
	\centering
	\label{sec:Recherche:Zielgruppe:top10}
	
	\begin{tabular}{ | c | l | l | l | } 
		\hline 
		\textbf{\#} & \textbf{Citytrip} & \textbf{Flug} & \textbf{Hotel} \\ \hline 
		1 & Berlin & Bangkok & London \\ \hline
		2 & Barcelona & Byculla & Rom \\ \hline
		3 & Wien & London & Paris \\ \hline
		4 & London & Berlin & New York City \\ \hline
		5 & Amsterdam & Miami & Berlin \\ \hline
		6 & Rom & Palma de Mallorca & Barcelona \\ \hline
		7 & Lissabon & Los Angeles & München \\ \hline
		8 & Hamburg & Wien  & Palma de Mallorca \\ \hline
		9 & Prag & San Francisco & Wien \\ \hline
		10 & Istanbul & Barcelona & Venedig \\ \hline
	\end{tabular} 
\end{table}

Es ist demnach wichtig, dass die oben genannten Browser getestet werden sowie die aufgeführten Destinationen.